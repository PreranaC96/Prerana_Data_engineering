{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "preranasmall",
              "session_id": "25",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-02-19T16:41:10.8472137Z",
              "session_start_time": "2024-02-19T16:41:10.8946546Z",
              "execution_start_time": "2024-02-19T16:41:47.4385017Z",
              "execution_finish_time": "2024-02-19T16:41:47.6001476Z",
              "spark_jobs": null,
              "parent_msg_id": "6aac3cca-b81b-4d9e-a120-c5484bca25d2"
            },
            "text/plain": "StatementMeta(preranasmall, 25, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql import Row\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "from datetime import *\r\n",
        "from delta.tables import *\r\n",
        " \r\n",
        "import pyspark.sql.functions as F\r\n",
        " \r\n",
        "#Debug\r\n",
        "Debug = True\r\n",
        "NoOfRecordsToDisplay = 1000\r\n",
        "SkipFCZeroGenerated = False\r\n",
        " \r\n",
        "#Set Spark Config\r\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\r\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\r\n",
        "spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\r\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",10485760)\r\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism * 2)\r\n",
        "#Config needed to read datetime column values without error [Don't remove]\r\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "preranasmall",
              "session_id": "25",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-02-19T16:41:10.909101Z",
              "session_start_time": null,
              "execution_start_time": "2024-02-19T16:41:47.7429481Z",
              "execution_finish_time": "2024-02-19T16:41:47.9019871Z",
              "spark_jobs": null,
              "parent_msg_id": "66c2f5f4-3143-45f9-b9d6-ae3b01902626"
            },
            "text/plain": "StatementMeta(preranasmall, 25, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "CuratedStorageAccountName = \"preranastorage007\"\r\n",
        "CuratedContainerName = \"casestudy\"\r\n",
        "beginDate = '2003-01-01'\r\n",
        "endDate = '2005-12-31'\r\n",
        "CuratedFolderName = \"curated/DimDate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "preranasmall",
              "session_id": "25",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-02-19T16:41:10.9963026Z",
              "session_start_time": null,
              "execution_start_time": "2024-02-19T16:41:48.0434351Z",
              "execution_finish_time": "2024-02-19T16:41:48.5681322Z",
              "spark_jobs": null,
              "parent_msg_id": "07d0b0ea-f496-4274-bf4c-c6e683d85233"
            },
            "text/plain": "StatementMeta(preranasmall, 25, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimNoDeltaTable: False\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Set Curated filepath \r\n",
        "CuratedADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s/' % (CuratedContainerName,CuratedStorageAccountName, CuratedFolderName)\r\n",
        "try:\r\n",
        "    DimNoDeltaTable = False\r\n",
        "    Result = mssparkutils.fs.ls(CuratedADLSPath + \"/_delta_log\")\r\n",
        "except:\r\n",
        "    DimNoDeltaTable = True\r\n",
        " \r\n",
        "if Debug == True:\r\n",
        "    print(\"DimNoDeltaTable: \" + str(DimNoDeltaTable))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "preranasmall",
              "session_id": "25",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-02-19T16:41:11.1325062Z",
              "session_start_time": null,
              "execution_start_time": "2024-02-19T16:41:48.69909Z",
              "execution_finish_time": "2024-02-19T16:41:48.8535782Z",
              "spark_jobs": null,
              "parent_msg_id": "be3233fb-6dbd-4508-b190-307f1f98459d"
            },
            "text/plain": "StatementMeta(preranasmall, 25, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if DimNoDeltaTable == True:\r\n",
        "  (\r\n",
        "    spark.sql(f\"select explode(sequence(to_date('{beginDate}'), to_date('{endDate}'), interval 1 day)) as calendarDate\")\r\n",
        "      .createOrReplaceTempView('TempTable_Dates')\r\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "preranasmall",
              "session_id": "25",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-02-19T16:41:11.297239Z",
              "session_start_time": null,
              "execution_start_time": "2024-02-19T16:41:48.9882339Z",
              "execution_finish_time": "2024-02-19T16:41:49.140986Z",
              "spark_jobs": null,
              "parent_msg_id": "c3dba760-becc-43a5-bf7a-7702abf9085e"
            },
            "text/plain": "StatementMeta(preranasmall, 25, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        " \r\n",
        " \r\n",
        "if DimNoDeltaTable == True:\r\n",
        "        DF_DimDate = spark.sql(\"\"\"\r\n",
        "                                        SELECT\r\n",
        "                                        year(d.CALENDARDATE) * 10000 + month(d.CALENDARDATE) * 100 + day(d.CALENDARDATE) AS DATEKEY,\r\n",
        "                                        d.CALENDARDATE AS CALENDARDATE,\r\n",
        "                                        date_format(d.CALENDARDATE, 'dd/MM/yyyy') AS DATE,\r\n",
        "                                        year(d.CALENDARDATE) AS CALENDARYEAR,\r\n",
        "                                        date_format(d.CALENDARDATE, 'MMMM') AS CALENDARMONTH,\r\n",
        "                                        month(d.CALENDARDATE) as MONTHOFYEAR,\r\n",
        "                                        date_format(d.CALENDARDATE, 'EEEE') AS CALENDARDAY,\r\n",
        "                                        dayofweek(d.CALENDARDATE) AS DAYOFWEEK,                                \r\n",
        "                                        dayofmonth(d.CALENDARDATE) AS DAYOFMONTH,\r\n",
        "                                        dayofyear(d.CALENDARDATE) AS DAYOFYEAR,\r\n",
        "                                        weekofyear(d.CALENDARDATE) AS WEEKOFYEARISO,\r\n",
        "                                        quarter(d.CALENDARDATE) AS QUARTEROFYEAR,\r\n",
        "                                        CASE WHEN (month(d.CALENDARDATE) > 3) THEN (year(d.CALENDARDATE) + 1) ELSE (year(d.CALENDARDATE)) END AS FINANCIALYEAR,\r\n",
        "                                        CASE WHEN (month(d.CALENDARDATE) > 3) THEN (month(d.CALENDARDATE) - 3) \r\n",
        "                                             WHEN (month(d.CALENDARDATE) = 1) THEN 10 \r\n",
        "                                             WHEN (month(d.CALENDARDATE) = 2) THEN 11 \r\n",
        "                                             WHEN (month(d.CALENDARDATE) = 3) THEN 12 \r\n",
        "                                             ELSE 00 END AS FINANCIALYEARMONTH,\r\n",
        "                                        CASE WHEN (month(d.CALENDARDATE) > 3 AND month(d.CALENDARDATE) <= 6 ) THEN 1 \r\n",
        "                                             WHEN (month(d.CALENDARDATE) >= 7 AND month(d.CALENDARDATE) <= 9 ) THEN 2 \r\n",
        "                                             WHEN (month(d.CALENDARDATE) >= 10 AND month(d.CALENDARDATE) <= 12 ) THEN 3\r\n",
        "                                             ELSE 4 END AS FINANCIALYEARQUARTER,\r\n",
        "                                        date_format(concat(\r\n",
        "                                                CASE WHEN (month(d.CALENDARDATE) > 3) THEN (year(d.CALENDARDATE) + 1) ELSE (year(d.CALENDARDATE)) END ,\r\n",
        "                                                '-04-01T00:00:00.0000000'), 'dd/MM/yyyy') AS FINANCIALYEARSTARTDATE, \r\n",
        "                                        CASE WHEN (dayofweek(d.CALENDARDATE) > 1) THEN date_add(date_add(d.CALENDARDATE,7-(dayofweek(d.CALENDARDATE))),1) ELSE d.CALENDARDATE END AS WEEKENDING,\r\n",
        "                                        current_timestamp() AS Lastupdated      \r\n",
        "                                        FROM\r\n",
        "                                                TempTable_Dates d\r\n",
        "                                        ORDER BY\r\n",
        "                                        d.CALENDARDATE;\r\n",
        "                                \"\"\")\r\n",
        "        DF_DimDate.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(CuratedADLSPath)\r\n",
        " \r\n",
        "        #Display Dataframe Result\r\n",
        "        if Debug == True:\r\n",
        "                print(\"Total Records: \" + str(DF_DimDate.count()))\r\n",
        "                display(DF_DimDate.limit(20))  "
      ]
    }
  ]
}